{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36c72e4e",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adf9317",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T12:12:27.673743Z",
     "start_time": "2025-02-15T12:12:27.667187Z"
    }
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from langchain_google_genai import GoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "import fitz  # PyMuPDF\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# Set your Google Gemini API key\n",
    "GOOGLE_API_KEY = \"GOOGLE_API_KEY\"  # Replace with your actual key\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16607882",
   "metadata": {},
   "source": [
    "### Extract Text from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf94857f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T12:03:28.449920Z",
     "start_time": "2025-02-15T12:03:28.427090Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts text from a given PDF file.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\") + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Upload your PDF file manually in Jupyter and provide the file path\n",
    "pdf_path = \"FILE.pdf\"  # Change to your actual file path\n",
    "pdf_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "print(\"âœ… PDF Text Extracted! Showing first 1000 characters:\\n\")\n",
    "print(pdf_text[:1000])  # Display only first 1000 characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c1e84c",
   "metadata": {},
   "source": [
    "### Split Extracted Text into Meaningful Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c557b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T12:03:32.905262Z",
     "start_time": "2025-02-15T12:03:32.898380Z"
    }
   },
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"Splits text into chunks while preserving context.\"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "# Split the extracted PDF text into chunks\n",
    "chunks = chunk_text(pdf_text)\n",
    "print(f\"âœ… Text Chunked! Total Chunks: {len(chunks)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70f268e",
   "metadata": {},
   "source": [
    "### Generate Embeddings with Google Gemini and Store in FAISS for Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16708978",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T12:03:36.855763Z",
     "start_time": "2025-02-15T12:03:34.712393Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use Google Gemini for embeddings\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# Convert text chunks into embeddings and store in FAISS\n",
    "vector_store = FAISS.from_texts(chunks, embedding_model)\n",
    "\n",
    "# Save FAISS index (optional)\n",
    "vector_store.save_local(\"faiss_index\")\n",
    "print(\"âœ… FAISS Index Created & Saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35364bc",
   "metadata": {},
   "source": [
    "### Load FAISS Index and Set Up Conversational Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acc978e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T12:03:38.453758Z",
     "start_time": "2025-02-15T12:03:38.440435Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load FAISS index with safe deserialization\n",
    "faiss_index = FAISS.load_local(\"faiss_index\", embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "# Initialize conversational memory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Create conversational retrieval chain\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    GoogleGenerativeAI(model=\"gemini-pro\", google_api_key=GOOGLE_API_KEY),\n",
    "    retriever=faiss_index.as_retriever(),\n",
    "    memory=memory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049ff39c",
   "metadata": {},
   "source": [
    "### Retrieve Relevant Information and Answer Questions with Google Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6a1fdc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T12:04:33.897186Z",
     "start_time": "2025-02-15T12:04:24.740784Z"
    }
   },
   "outputs": [],
   "source": [
    "def retrieve_and_answer(query, vector_store, full_text):\n",
    "    \"\"\"Retrieves relevant context OR provides a summary when requested.\"\"\"\n",
    "    \n",
    "    # If the user asks for a summary, provide the full document\n",
    "    summary_keywords = [\"summary\", \"summarize\", \"brief\", \"overview\"]\n",
    "    if any(keyword in query.lower() for keyword in summary_keywords):\n",
    "        context = full_text  # Use full text for summarization\n",
    "    else:\n",
    "        # Find relevant documents\n",
    "        relevant_docs = vector_store.similarity_search(query, k=3)  # Retrieve top 3 most relevant chunks\n",
    "        context = \"\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "\n",
    "    # Generate response using Gemini\n",
    "    llm = GoogleGenerativeAI(model=\"gemini-pro\", google_api_key=GOOGLE_API_KEY)\n",
    "    prompt = f\"Based on the following document, answer the question:\\n\\n{context}\\n\\nQuestion: {query}\"\n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Example Usage\n",
    "user_query = input(\"Ask a question: \")\n",
    "answer = retrieve_and_answer(user_query, faiss_index, pdf_text)\n",
    "print(\"\\nðŸ’¡ Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64b2005",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
